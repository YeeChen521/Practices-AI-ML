{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import ssd300_vgg16, SSD300_VGG16_Weights\n",
    "from torchvision.models.detection.ssd import SSDClassificationHead\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 38\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c91ee9",
   "metadata": {},
   "source": [
    "1. Create a custom PyTorch Dataset for object detectiong using COCO-format annotations\n",
    "- `__init__`: store paths, load annotations, prepare label mapping\n",
    "    - use lazy loading(memory efficient)\n",
    "    - `COCO()`: parse JSON,build fast lookup tables(include images, annotations, categories)\n",
    "    - `self.reverse_label_map`: used for evaluation, visualization and mapping prediction back to COCO IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split, transforms=None):\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "        self.dir = os.path.join(root, \"Senior-Design-VIAD-4\", split)\n",
    "        \n",
    "        ann_path = os.path.join(self.dir, \"_annotations.coco.json\")\n",
    "        self.coco = COCO(ann_path)\n",
    "        \n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "        cat_ids = sorted(self.coco.getCatIds())\n",
    "        # SSD reserves label 0 for background\n",
    "        # avoid CUDA asserts errors during training\n",
    "        self.label_map = {cat_id: i + 1 for i, cat_id in enumerate(cat_ids)}\n",
    "        \n",
    "        self.reverse_label_map = {v: k for k, v in self.label_map.items()}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        \n",
    "        # anns is a list of objects\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # load the image\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img = Image.open(os.path.join(self.dir, img_info[\"file_name\"])).convert(\"RGB\")\n",
    "\n",
    "        # bounding box processing\n",
    "        boxes, labels = [], []\n",
    "\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            if w > 1 and h > 1:\n",
    "                # convert the COCO bbox format to SSD format\n",
    "                boxes.append([x, y, x + w, y + h])\n",
    "                # convert COCO category IDs to contiguous labels\n",
    "                labels.append(self.label_map[ann[\"category_id\"]])\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor(img_id)\n",
    "        }\n",
    "    \n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0937ecdf",
   "metadata": {},
   "source": [
    "2. Image Preprocessing\n",
    "- `get_tranform`:\n",
    "    - convert PIL image to PyTorch Tensor\n",
    "    - pixel range: `[0,255] -> [0.0,1.0]`\n",
    "    - shape: `(H,W,C) -> (C,H,W)`\n",
    "- `collate_fn`:\n",
    "    - object detection data cannot be stacked normally\n",
    "    - inputs: \n",
    "    ```\n",
    "    [\n",
    "        (img1, target1),\n",
    "        (img2, target2)\n",
    "    ]\n",
    "    ```\n",
    "    - outptus:\n",
    "    ```\n",
    "    (\n",
    "        (img1, img2),\n",
    "        (target1, target2)\n",
    "    )\n",
    "    ```\n",
    "    - required for variable-length annotations in detection tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d71af",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"mohamedgobara/26-class-object-detection-dataset\")\n",
    "\n",
    "def get_transform():\n",
    "    return transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# creating dataset objects\n",
    "train_dataset = UrbanDataset(path, \"train\", get_transform())\n",
    "valid_dataset = UrbanDataset(path, \"valid\", get_transform())\n",
    "test_dataset  = UrbanDataset(path, \"test\",  get_transform())\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# used for fast debugging\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, list(range(200)))\n",
    "valid_dataset = torch.utils.data.Subset(valid_dataset, list(range(50)))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    # prevent learning order bias\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    # load data in parallel, speed up training\n",
    "    num_workers=4,\n",
    "    # improve GPU transfer efficiency      \n",
    "    pin_memory=True     \n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # validation should be deterministic, reproducible and order independent\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b613f",
   "metadata": {},
   "source": [
    "3. Setup model\n",
    "- COCO weights: the model already understands edges,shapes and generic objects\n",
    "- `num_anchors_per_location()`: return a list of integers, one per feature map\n",
    "- anchor count determines how many predictions are made per feature map location\n",
    "- change the original COCO head(91 classes) to `NUM_CLASSES`\n",
    "- total outputs per location = `num_anchors` * `NUM_CLASSES`\n",
    "- only replacing the classification head\n",
    "- backbone: extracts features from the images\n",
    "- anchor: predefined box placed on the images\n",
    "- anchor generator: decides anchor sizes, aspect ratios and how many anchors per location\n",
    "- regression head: predict how to move and resize each anchor box\n",
    "\n",
    "```\n",
    "SSD has fixed anchors\n",
    "\n",
    "Each anchor predicts a class\n",
    "\n",
    "Number of classes must match dataset\n",
    "\n",
    "Classification head defines that mapping\n",
    "```\n",
    "### End-to-end-flow\n",
    "Image\n",
    " ↓\n",
    "Backbone (VGG16)\n",
    " ↓\n",
    "Feature maps\n",
    " ↓\n",
    "Anchor Generator → anchors\n",
    " ↓\n",
    "Regression Head → box offsets\n",
    " ↓\n",
    "Classification Head → class scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = SSD300_VGG16_Weights.COCO_V1\n",
    "model = ssd300_vgg16(weights=weights)\n",
    "\n",
    "num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "\n",
    "# channel depth of the six feature maps SSD uses\n",
    "# comes from VGG16 backbone and extra SSD convolution layers\n",
    "in_channels = [512, 1024, 512, 256, 256, 256]\n",
    "\n",
    "model.head.classification_head = SSDClassificationHead(\n",
    "    in_channels,\n",
    "    num_anchors,\n",
    "    NUM_CLASSES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03c6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks dataset labels to ensure they are within the expected range\n",
    "max_label = 0\n",
    "for i in range(len(train_dataset)):\n",
    "    _, t = train_dataset[i]\n",
    "    # only check images that have at least one object\n",
    "    if t[\"labels\"].numel() > 0:\n",
    "        max_label = max(max_label, t[\"labels\"].max().item())\n",
    "\n",
    "assert max_label < NUM_CLASSES, \"❌ Label exceeds NUM_CLASSES\"\n",
    "print(\"✅ Dataset labels valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852babc6",
   "metadata": {},
   "source": [
    "4. Training\n",
    "```\n",
    "For each epoch:\n",
    "    Train:\n",
    "        For each batch:\n",
    "            Move data → device\n",
    "            Forward pass → compute loss\n",
    "            Backprop → update weights\n",
    "            Accumulate loss\n",
    "    Validate:\n",
    "        Move data → device\n",
    "        Forward pass only (no backprop)\n",
    "        Accumulate loss\n",
    "    Print average train & val loss\n",
    "\n",
    "```\n",
    "\n",
    "** BatchNorm forces the input to have zero mean and unit variance. It stabilizes training, allows higher learning rates and acts as a slight regularizer.\n",
    "It normalizes the inputs of each layer across a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b29ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"✅ CUDA is available! Using GPU.\")\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"❌ CUDA is not available. Using CPU.\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Stochastic Gradient Descent(SGD)\n",
    "# momentum: speed up convergence\n",
    "# weight_decay: L2 regularization\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad() # clear previous gradients\n",
    "        loss.backward() # compute gradients w.r.t paramaters\n",
    "        optimizer.step() # update weights\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # validation\n",
    "    model.eval() # disables dropout and BatchNorm updates\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad(): # save memory & speed up\n",
    "        for images, targets in valid_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            val_loss += sum(loss_dict.values()).item()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "        f\"Train: {train_loss/len(train_loader):.4f} \"\n",
    "        f\"Val: {val_loss/len(valid_loader):.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a77313",
   "metadata": {},
   "source": [
    "5. Testing\n",
    "```\n",
    "Switch model to eval mode\n",
    "\n",
    "Load a single image\n",
    "\n",
    "Forward pass with no gradients\n",
    "\n",
    "Filter predictions by confidence\n",
    "\n",
    "Draw boxes and labels on image\n",
    "\n",
    "Show the image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3254ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only show predictions the model is confident about\n",
    "def predict(model, dataset, index=0, threshold=0.5):\n",
    "    model.eval()\n",
    "    # get the image\n",
    "    img, _ = dataset[index]\n",
    "\n",
    "    # forward pass without gradient\n",
    "    with torch.no_grad():\n",
    "        output = model([img.to(device)])[0]\n",
    "\n",
    "    # prepare image for ploting\n",
    "    img_np = img.permute(1, 2, 0).numpy() #PyTorch -> matplotlib\n",
    "    plt.imshow(img_np)\n",
    "    ax = plt.gca() # get current axis for adding boxes\n",
    "\n",
    "    # loop through predictions\n",
    "    for box, label, score in zip(\n",
    "        output[\"boxes\"], output[\"labels\"], output[\"scores\"]\n",
    "    ):\n",
    "        if score > threshold:\n",
    "            x1, y1, x2, y2 = box.cpu().numpy()\n",
    "            ax.add_patch(\n",
    "                plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                              fill=False, edgecolor=\"red\", linewidth=2)\n",
    "            )\n",
    "            ax.text(x1, y1, f\"{label.item()} ({score:.2f})\",\n",
    "                    bbox=dict(facecolor=\"yellow\", alpha=0.5))\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "predict(model, test_dataset, index=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bfa23a",
   "metadata": {},
   "source": [
    "6. Evaluate Metrics\n",
    "```\n",
    "For each batch:\n",
    "    Move images to device\n",
    "    Forward pass → predictions\n",
    "    Convert boxes to COCO format\n",
    "Aggregate predictions\n",
    "Use pycocotools.COCOeval to compute mAP, AP50, AP75, AR\n",
    "Print summary\n",
    "```\n",
    "\n",
    "** Ground Truth boxes(GT boxes) is a bounding box that encapsulates an object in the image and comes with a class label\n",
    "- Compute loss\n",
    "- Regression loss\n",
    "```\n",
    "Image → model predicts boxes\n",
    "GT boxes → true answer\n",
    "Compare → measure how well model predicted\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90608de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    # loop over batches\n",
    "    for images, targets in loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "        # loop through predictions per images\n",
    "        for target, output in zip(targets, outputs):\n",
    "            img_id = int(target[\"image_id\"])\n",
    "            for box, score, label in zip(\n",
    "                output[\"boxes\"], output[\"scores\"], output[\"labels\"]\n",
    "            ):\n",
    "                x1, y1, x2, y2 = box.cpu().numpy()\n",
    "                # convert predictions to COCO JSON format\n",
    "                results.append({\n",
    "                    \"image_id\": img_id,\n",
    "                    \"category_id\": int(label),  \n",
    "                    \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
    "                    \"score\": float(score),\n",
    "                })\n",
    "                \n",
    "    # create COCO results objects\n",
    "    # convert results to COCO evaluation object\n",
    "    coco_dt = loader.dataset.coco.loadRes(results)\n",
    "    coco_eval = COCOeval(loader.dataset.coco, coco_dt, \"bbox\")\n",
    "    \n",
    "    # matches predictions to GT boxes\n",
    "    coco_eval.evaluate()\n",
    "    \n",
    "    # computes precision & recall across IoU thresolds\n",
    "    coco_eval.accumulate()\n",
    "    \n",
    "    # print metrics(mean average precision(mAP), AP@0.50,AP@0.75,average recall(AR))\n",
    "    coco_eval.summarize()\n",
    "\n",
    "test_dataset = torch.utils.data.Subset(test_dataset, list(range(50)))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "evaluate_model(model, test_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
